{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL(wiki, kaggle, rating):\n",
    "    # Extract data using file directory and commands\n",
    "    file_dr='/Users/danielchang/Desktop/UCB/Module_8/Movies_ETL/'\n",
    "    with open(f'{file_dr}wikipedia.movies.json', mode='r') as file:\n",
    "        wiki_movies_raw=json.load(file)\n",
    "\n",
    "    kaggle_metadata=pd.read_csv(f'{file_dr}movies_metadata.csv')\n",
    "    ratings=pd.read_csv(f'{file_dr}ratings.csv')\n",
    "\n",
    "    # create wiki dataframe \n",
    "    wiki_movies_df=pd.DataFrame(wiki_movies_raw)\n",
    "\n",
    "    # Transform wiki by deleting tv shows\n",
    "    wiki_movies=[movie for movie in wiki_movies_raw\n",
    "                if ('Director' in movie or 'Directed by' in movie)\n",
    "                and 'imdb_link' in movie\n",
    "                and 'No. of episodes' not in movie]\n",
    "\n",
    "    # Create an alternate title column while deleting unneccassry titles\n",
    "    def clean_movie(movie):\n",
    "        movie=dict(movie) # create a non-destructive copy\n",
    "        alt_titles={}\n",
    "        for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                    'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                    'Mandarin','McCuneâ€“Reischauer','Original title','Polish',\n",
    "                    'Revised Romanization','Romanized','Russian',\n",
    "                    'Simplified','Traditional','Yiddish']:\n",
    "            if key in movie:\n",
    "                alt_titles[key]=movie[key]\n",
    "                movie.pop(key)\n",
    "        if len(alt_titles)>0:\n",
    "            movie['alt_titles']=alt_titles\n",
    "\n",
    "        # merge column names\n",
    "        def change_column_name(old_name, new_name):\n",
    "            if old_name in movie:\n",
    "                movie[new_name]=movie.pop(old_name)\n",
    "        change_column_name('Adaptation by', 'Writer(s)')\n",
    "        change_column_name('Country of origin', 'Country')\n",
    "        change_column_name('Directed by', 'Director')\n",
    "        change_column_name('Distributed by', 'Distributor')\n",
    "        change_column_name('Edited by', 'Editor(s)')\n",
    "        change_column_name('Length', 'Running time')\n",
    "        change_column_name('Original release', 'Release date')\n",
    "        change_column_name('Music by', 'Composer(s)')\n",
    "        change_column_name('Produced by', 'Producer(s)')\n",
    "        change_column_name('Producer', 'Producer(s)')\n",
    "        change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "        change_column_name('Productioncompany ', 'Production company(s)')\n",
    "        change_column_name('Released', 'Release Date')\n",
    "        change_column_name('Release Date', 'Release date')\n",
    "        change_column_name('Screen story by', 'Writer(s)')\n",
    "        change_column_name('Screenplay by', 'Writer(s)')\n",
    "        change_column_name('Story by', 'Writer(s)')\n",
    "        change_column_name('Theme music composer', 'Composer(s)')\n",
    "        change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "        return movie\n",
    "\n",
    "    # Use the clean_movie function\n",
    "    clean_movies =[clean_movie(movie) for movie in wiki_movies]\n",
    "    wiki_movies_df=pd.DataFrame(clean_movies)\n",
    "\n",
    "    # Extract the bad data\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id']=wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    \n",
    "    except:\n",
    "        print(\"Error with imdb_link\")\n",
    "\n",
    "    # Keeping important columns\n",
    "    wiki_columns_to_keep=[column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df)*0.9]\n",
    "    wiki_movies_df=wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "    # Edit box office data type\n",
    "    box_office=wiki_movies_df['Box office'].dropna()\n",
    "\n",
    "    box_office[box_office.map(lambda x: type(x) != str)]\n",
    "\n",
    "    box_office=box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    form_one=r'\\$\\s*\\d+\\.?\\d*\\s[mb]illi?on'\n",
    "    form_two=r'\\$\\s*\\d{1,3}(?:[,\\.],\\d{3})+(?!\\s[mb]illion)'\n",
    "    box_office = box_office.str.replace(r'\\$.*[---](?![a-z])','$', regex=True)\n",
    "    box_office.str.extract(f'({form_one}|{form_two})')\n",
    "\n",
    "    # function to edit data\n",
    "    def parse_dollars(s):\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*million', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','',s)\n",
    "            value=float(s) * 10**6\n",
    "            return value\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billion', s, flags=re.IGNORECASE):\n",
    "            s=re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            value=float(s) * 10**9\n",
    "            return value\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "            s=re.sub('\\$|,','',s)\n",
    "            value=float(s)\n",
    "            return value\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "\n",
    "    # Edit budget data type\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget=budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget=budget.str.replace(r'\\$.*[---](?![a-z])', '$', regex=True)\n",
    "\n",
    "    matches_form_one=budget.str.contains(form_one, flags=re.IGNORECASE)\n",
    "    matches_form_two=budget.str.contains(form_two, flags=re.IGNORECASE)\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "\n",
    "    budget=budget.str.replace(r'\\[\\d+\\]s*','')\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "\n",
    "    wiki_movies_df['budget']=budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "\n",
    "    # Edit release date data type\n",
    "    release_date=wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    date_form_one=r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "\n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "\n",
    "    wiki_movies_df['release_date']= pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "\n",
    "    # Edit running time data type\n",
    "    running_time=wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE).sum()\n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE) != True]\n",
    "\n",
    "    running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE).sum()\n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE) != True]\n",
    "\n",
    "    running_time_extract=running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract=running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "\n",
    "    wiki_movies_df['running_time']=running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "\n",
    "    # Drop Kaggle data\n",
    "    kaggle_metadata=kaggle_metadata[kaggle_metadata['adult']=='False'].drop('adult',axis='columns')\n",
    "\n",
    "    kaggle_metadata['video']=kaggle_metadata['video']=='True'\n",
    "    kaggle_metadata['budget']=kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id']=pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity']=pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    kaggle_metadata['release_date']=pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "    # Datetime Stamp\n",
    "    ratings['timstamp']=pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # Find out the index of the outlier row\n",
    "    movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index\n",
    "    movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "\n",
    "    # Convert the language data to tuples\n",
    "    movies_df['Language'].apply(lambda x: tuple(x) if type(x) == list else x).value_counts(dropna=False)\n",
    "    \n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    # Make a function that fills in missing data for a column pair and then drops the redundant column.\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "    for col in movies_df.columns:\n",
    "        lists_to_tuples=lambda x: tuple(x) if type(x) == list else x\n",
    "        value_counts=movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "        num_values=len(value_counts)\n",
    "        if num_values==1:\n",
    "            print(col)\n",
    "\n",
    "    movies_df['video'].value_counts(dropna=False)\n",
    "\n",
    "    # Edit Final DF\n",
    "    movies_df = movies_df[['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                          ]]\n",
    "\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "\n",
    "    rating_counts=ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "    .rename({'userId':'count'}, axis=1) \\\n",
    "    .pivot(index='movieId', columns='rating', values='count')\n",
    "\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "\n",
    "    movies_with_ratings_df= pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "\n",
    "    movies_with_ratings_df[rating_counts.columns]=movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "\n",
    "    # Load wiki and kaggle to sql\n",
    "    db_string=f'postgres://postgres:{db_password}@127.0.0.1:5432/movie_data'\n",
    "    engine=create_engine(db_string)\n",
    "    movies_df.to_sql(name='movies', con=engine)\n",
    "\n",
    "    # Load ratings data to sql\n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'ratings.csv', chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "        \n",
    "    print('ETL process complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielchang/anaconda3/envs/PythonData/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3242: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\n",
      "importing rows 0 to 1000000...Done. 156.35444808006287 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 307.6146469116211 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 433.44512915611267 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 553.2551140785217 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 681.8885898590088 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 802.022577047348 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 921.4888291358948 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 1042.16903591156 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 1161.4182081222534 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 1282.320482969284 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 1401.777941942215 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 1527.4728798866272 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1650.651160955429 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1771.421557188034 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1895.2163870334625 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 2014.2769260406494 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 2134.8105738162994 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 2255.6728501319885 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 2376.2351200580597 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 2499.472753047943 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 2620.405478000641 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 2741.0982699394226 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 2862.185269832611 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 2984.553272008896 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 3108.9026930332184 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 3230.995141029358 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 3233.869359970093 total seconds elapsed\n",
      "ETL process complete.\n"
     ]
    }
   ],
   "source": [
    "ETL('wikipedia.movies.json', 'movies_metadata.csv', 'ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
